{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75818d7e-3d38-4748-a759-a96045e1ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import copy\n",
    "import itertools\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "import pyttsx3\n",
    "\n",
    "# Set the path for Tesseract OCR executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Initialize the text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Load the saved model from file\n",
    "model = keras.models.load_model(\"model.h5\")\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# New folder names for gestures\n",
    "gesture_labels = ['bathroom', 'hello', 'help', 'iloveyou', 'more', 'no', 'repeat', 'thanks', 'yes']\n",
    "\n",
    "# Functions\n",
    "def calc_landmark_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "    landmark_point = []\n",
    "\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "\n",
    "    return landmark_point\n",
    "\n",
    "def pre_process_landmark(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, landmark_point in enumerate(temp_landmark_list):\n",
    "        if index == 0:\n",
    "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "\n",
    "        temp_landmark_list[index][0] -= base_x\n",
    "        temp_landmark_list[index][1] -= base_y\n",
    "\n",
    "    temp_landmark_list = list(itertools.chain.from_iterable(temp_landmark_list))\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "\n",
    "    def normalize_(n):\n",
    "        return n / max_value\n",
    "\n",
    "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "\n",
    "    return temp_landmark_list\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        image = cv2.flip(image, 1)\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        debug_image = copy.deepcopy(image)\n",
    "\n",
    "        detected_labels = []\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                landmark_list = calc_landmark_list(debug_image, hand_landmarks)\n",
    "                pre_processed_landmark_list = pre_process_landmark(landmark_list)\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style())\n",
    "                df = pd.DataFrame(pre_processed_landmark_list).transpose()\n",
    "\n",
    "                # Predict the gesture\n",
    "                predictions = model.predict(df, verbose=0)\n",
    "                predicted_class = np.argmax(predictions, axis=1)\n",
    "                label = gesture_labels[predicted_class[0]]\n",
    "\n",
    "                # Modify 'iloveyou' to 'I love you' for speech output\n",
    "                if label == 'iloveyou':\n",
    "                    label = 'I love you'\n",
    "\n",
    "                detected_labels.append(label)\n",
    "                cv2.putText(image, label, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 2)\n",
    "                print(label)\n",
    "\n",
    "        # Use Tesseract OCR to extract text from the image\n",
    "        extracted_text = pytesseract.image_to_string(image)\n",
    "        print(\"Extracted Text:\", extracted_text)\n",
    "\n",
    "        # Combine detected labels and extracted text\n",
    "        combined_text = ' '.join(detected_labels) + ' ' + extracted_text\n",
    "\n",
    "        # Read the combined text aloud\n",
    "        engine.say(combined_text)\n",
    "        engine.runAndWait()\n",
    "\n",
    "        # Output image\n",
    "        cv2.imshow('Indian sign language detector', image)\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d854a-af9c-44c0-bcff-0b4011ffd790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
